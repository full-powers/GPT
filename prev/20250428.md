# translate English sentence to Japanese
# only translate the sentence to Japanese after the "--" line
# and add a new line after the Japanese sentence

it is the landmark matrices used for obtaining the approximation.

--

それは近似を取得するために使用されるランドマーク行列です。

-

Q˜ and K˜ are matrices formed by sets of m landmarks,
computed as the segment-means of the matrices Q and K,
respectively.

--

Q˜とK˜は、mランドマークのセットによって形成された行列であり、
それぞれ、行列QとKのセグメント平均として計算されます。

-

LeanAttention is an optimized scalable attention execution mechanism. It provides extensive parallelism across all modes of the attention tensor, with well-balanced computation workloads to each CTA ensuring close to 100% SM occupancy while delivering a runtime speedup in attention execution.

--

LeanAttentionは、最適化されたスケーラブルな注意実行メカニズムです。
すべての注意テンソルモードにわたって広範な並列性を提供し、
各CTAに対して計算ワークロードを適切にバランスさせることで、100%
SM占有率に近い
ながら、注意実行のランタイムスピードアップを提供します。

-

First, we identify the smallest optimal granularity of decomposition in attention computation, termed as LeanTile (subsection IV-B), which can be linearly mapped on the hardware
resources in a flexible style akin to stream-k decomposition
of matrix multiplications subsection IV-C). Multiple such
LeanTiles belonging to either single or multiple attention
outputs will constitute a workload assigned to a CTA. By
the nature of Stream-K’s equalized load balancing strategy,
each CTA will compute equal number of LeanTiles, ensuring
no idle SMs during the entire duration of attention computation.

--

まず、注意計算における分解の最小の最適粒度を特定します。
LeanTile（サブセクションIV-B）と呼ばれ、
ハードウェアリソースにストリーム-k分解のような柔軟なスタイルで
線形的にマッピングできます
（行列乗算のサブセクションIV-C）。
単一または複数の注意のいずれかに属するこのような
LeanTileが複数存在します。
単一または複数の注意
出力は、CTAに割り当てられるワークロードを構成します。
ストリーム-Kの均等化された負荷分散戦略の性質により、
各CTAは同じ数のLeanTileを計算し、
注意計算の全期間中に    アイドルSMがないことを保証します。
-

In LeanAttention, we propose computation of partial attention outputs of a given query tile concurrently on different hardware units, while ensuring that we have a well-balanced work
distribution across all hardware units through a Stream-K style
decomposition of attention (discussed later in subsection IV-C).
This decomposition results in splits of work for a given SM
that are not always equal in size, i.e., the key/value tensors
of a given query tile are not dispatched in same-sized blocks
to different SMs

--
LeanAttentionでは、異なるハードウェアユニットで
与えられたクエリタイルの部分的な注意出力を同時に計算することを提案します。
すべてのハードウェアユニットにわたって
適切にバランスの取れた作業を保証しながら
分布
ストリーム-Kスタイルの
注意（後でサブセクションIV-Cで説明します）。
この分解は、与えられたSMの作業の分割をもたらします。
常にサイズが等しいわけではありません。
つまり、与えられたクエリタイルの
キー/値テンソルは、同じサイズのブロックで

-
To reduce these partial attention outputs that result from
differently sized blocks, we use a softmax re-scaling operation.
This requires us to identify softmax re-scaling’s associativity
property that allows it to correctly reduce blocks of unequal
sizes, i.e., application of softmax re-scaling as a reduction
operator will give the same exact attention output with no loss
in accuracy, regardless of the way the work might be split,
whether in same-sized blocks or arbitrary differently sized
blocks.

--
異なるサイズのブロックから得られるこれらの部分的な注意出力を
削減するために、
ソフトマックスの再スケーリング操作を使用します。
これにより、ソフトマックスの再スケーリングの結合性を特定する必要があります。
それにより、正しく不平等なブロックを削減できます。
つまり、ソフトマックスの再スケーリングを適用することは
削減
演算子は、作業の分割方法に関係なく、
同じ正確な注意出力を提供します。
精度の損失はありません。
分割方法に関係なく、
同じサイズのブロックまたは任意の異なるサイズのブロックで
-

 Illustrative diagram showing LeanAttention’s partitioning strategy with
two differently sized work volumes of a head assigned to different CTAs.
The un-scaled outputs are independently computed and re-scaled later in a
reduction operation. Note that this can be generalized to any arbitrary-sized
work volume split.
--
図は、異なるCTAに割り当てられたヘッドの2つの異なるサイズの
作業ボリュームを示すLeanAttentionの分割戦略を示しています。
スケーリングされていない出力は独立して計算され、
後で再スケーリングされます。
減算操作。
注意してください。これは、任意のサイズの
作業ボリューム分割に一般化できます。
-

