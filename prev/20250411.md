# translate English sentence to Japanese
# only translate the sentence to Japanese after the "--" line
# and add a new line after the Japanese sentence

What do we mean by auto regressive?
This term “auto regressive” in machine learning comes from time series.

In the context of deep learning, auto regressive models are a class of models that generate data points in a sequence by conditioning each new point on the previously generated points

The term autoregressive refers to a model or process where the current value is predicted based on its own past values

We have seen similar model before. Example Encoder Decoder Architecture

--

自己回帰とは何を意味するのでしょうか？
この「自己回帰」という用語は、機械学習の時間系列から来ています。
深層学習の文脈では、自己回帰モデルは、生成された各新しい点を以前に生成された点に条件付けることによって、シーケンス内のデータポイントを生成するモデルのクラスです。
自己回帰という用語は、現在の値が過去の値に基づいて予測されるモデルまたはプロセスを指します。
以前に似たようなモデルを見たことがあります。例：エンコーダデコーダアーキテクチャ

-

In the decoder block of the encoder decoder architecture, to produce the output at the next timestamp, it is taking hidden states and cell states of the previous timestamp and also the output of the previous timestamp.

Here to produce the output at any timestamp, we are taking help of the output generated at previous timestamp.

All seq2seq models are auto regressive.

Here these models generate output one at a time. Why is there no such model which can generate the whole output at a single time? It is because, while generating output at any time, the model should know what all it has generated before this output. Which means the current word depends on the previous or the past outputs.

Hence these models are auto regressive and it is impossible to generate the whole output all at once. Also the data here is sequential.

--

エンコーダデコーダアーキテクチャのデコーダブロックでは、次のタイムスタンプで出力を生成するために、前のタイムスタンプの隠れ状態とセル状態、および前のタイムスタンプの出力を取得しています。

ここで、任意のタイムスタンプで出力を生成するために、前のタイムスタンプで生成された出力の助けを借りています。

すべてのseq2seqモデルは自己回帰です。

ここでは、これらのモデルは1回で出力を生成します。なぜ、単一の時間で全体の出力を生成できるモデルがないのでしょうか？それは、任意の時間に出力を生成する際に、モデルがこの出力の前に何を生成したかを知っている必要があるからです。つまり、現在の単語は以前または過去の出力に依存しています。

したがって、これらのモデルは自己回帰であり、一度に全体の出力を生成することは不可能です。また、ここでのデータは連続しています。

-

That is why transformers are also auto regressive models

So we can understand that transformer decoder is autoregressive at inference time

--

だから、トランスフォーマーも自己回帰モデルです。

したがって、推論時にトランスフォーマーデコーダーは自己回帰であることを理解できます。

-

Now, lets look into why did we say that transformers are non auto regressive at training time
The reason behind this is masked self attention

Lets assume that transformers works as auto regressive during training time

Which means the decoder block of the transformer produces one output word at a time

During training of the decoder, we studied in the encoder decoder architecture a concept called “teacher forcing” which means that even if the output generated by the decoder at previous timestamp is wrong, for the next timestamp the correct output for that word will be sent as input instead of that wrong output at the previous timestamp

So now this process of giving output was also auto regressive as the output was being generated based on the values of the previous timestamp.

--

さて、トランスフォーマーがトレーニング時に非自己回帰であると言った理由を見てみましょう。

その理由は、マスク付き自己注意です。

トランスフォーマーがトレーニング時に自己回帰として機能すると仮定しましょう。

つまり、トランスフォーマーのデコーダーブロックは、一度に1つの出力単語を生成します。

デコーダーのトレーニング中に、エンコーダーデコーダーアーキテクチャで「教師強制」と呼ばれる概念を学びました。これは、デコーダーが前のタイムスタンプで生成した出力が間違っていても、次のタイムスタンプでは、その間違った出力の代わりに、その単語の正しい出力が入力として送信されることを意味します。

この出力を与えるプロセスは、前のタイムスタンプの値に基づいて出力が生成されていたため、自己回帰でもありました。

-

But here there is a problem
Because due to training process being auto regressive, our training process becomes very slow

--

しかし、ここに問題があります。
トレーニングプロセスが自己回帰であるため、トレーニングプロセスが非常に遅くなります。

-

But we made the training process as auto regressive because it is our necessity that we want to output sequential data, so auto regressive is the only option we have

--

しかし、出力シーケンシャルデータを必要とするため、トレーニングプロセスを自己回帰にしました。したがって、自己回帰は私たちが持っている唯一のオプションです。

-

In training we do not have any necessity that we need to be dependent on the output generated at previous timestamp. It is because we are using the concept of teacher forcing here. Here it does not matter if the output generated at previous time stamp is right or wrong, for the current time stamp we are sending the correct value only even if the output value at the previous time stamp was right or wrong. So now, there is no dependency of time stamp inside the decoder. And these operations inside the decoder can be run parallelly

The same happens inside the transformer as it is created in such a way that it is non auto regressive during training time

--

トレーニングでは、前のタイムスタンプで生成された出力に依存する必要はありません。教師強制の概念を使用しているためです。ここでは、前のタイムスタンプで生成された出力が正しいか間違っているかは関係ありません。現在のタイムスタンプでは、前のタイムスタンプの出力値が正しいか間違っていても、正しい値のみを送信しています。したがって、デコーダー内にタイムスタンプの依存関係はありません。そして、デコーダー内のこれらの操作は並行して実行できます。
同じことがトランスフォーマーの内部でも起こります。トランスフォーマーは、トレーニング時に非自己回帰になるように作られています。

-

Now, here in the decoder block, if we see carefully, there is a block called “Masked Multi Head Attention”. Lets just forget the term “masked” for some time and we know that Multi Head Attention is nothing but collection of multiple self attention blocks. So for simplicity purposes, we will assume it as “self attention” block

In self attention block, we say that each word’s embedding is formed as a mixture of other word’s embeddings from the sentence.

For example : If the sentence “My name is Abhishek” is getting passed to the self attention layer, then we can write that the embedding for the word ‘My” is a combination of the embeddings of the words “name”, “is”, and “abhishek”

--

さて、デコーダーブロックでは、「Masked Multi Head Attention」と呼ばれるブロックがあります。「マスク」という用語をしばらく忘れておきましょう。マルチヘッドアテンションは、複数の自己注意ブロックのコレクションに過ぎないことを知っています。したがって、単純化の目的で、「自己注意」ブロックとして仮定します。
自己注意ブロックでは、各単語の埋め込みは、文中の他の単語の埋め込みの混合として形成されると言います。
例えば、「My name is Abhishek」という文が自己注意層に渡される場合、単語「My」の埋め込みは、「name」、「is」、「abhishek」の単語の埋め込みの組み合わせであると書くことができます。

-


Here in the above image, each word from the input sentence “money bank grows” considers other words while creating its own embeddings

So we can write like this


FIGURE 1
From the above picture, we can say that the new embedding vector for the word “money” is made by using 70% of the embedding vector of the word “money”, 20% of the embedding vector of the word “bank” and 10% of the embedding vector of the word “grows”

So here each word’s embedding vector is getting represented as a combination of different words embedding vectors

--

上の画像では、入力文「money bank grows」の各単語が、独自の埋め込みを作成する際に他の単語を考慮しています。
したがって、次のように書くことができます。
FIGURE 1
上の図から、単語「money」の新しい埋め込みベクトルは、単語「money」の埋め込みベクトルの70％、単語「bank」の埋め込みベクトルの20％、単語「grows」の埋め込みベクトルの10％を使用して作成されると言えます。
したがって、ここでは、各単語の埋め込みベクトルが異なる単語の埋め込みベクトルの組み合わせとして表現されています。

-

But here, when the word “money’ was just written, to create its contextual embedding as we just saw, it is dependent on the word “bank” and “grows” which are basically the future inputs

Similarly, when the word “bank” was added to the sentence, to create its contextual embedding as we just saw, it is dependent on the word “grows” which is basically the future input

We can work with this approach during the training time as the dataset is there, but this will cause problem during inference time, we don’t know which word is gonna come next, so the equations from figure1 could be anything, so these equations becomes invalid as there are future terms written inside it. THIS IS THE EXAMPLE OF DATA LEAKAGE.

We would not have faced this problem of data leakage if we would have used the auto regressive approach or the sequential approach while training

So this is the problem we face when we use parallel approach

--

しかし、ここで、「money」という単語が書かれたとき、その文脈的な埋め込みを作成するために、先ほど見たように、「bank」と「grows」という基本的に未来の入力に依存しています。
同様に、「bank」という単語が文に追加されたとき、先ほど見たように、その文脈的な埋め込みを作成するために、「grows」という基本的に未来の入力に依存しています。
このアプローチはデータセットがあるため、トレーニング時に機能しますが、推論時には問題が発生します。次にどの単語が来るかわからないので、図1の方程式は何でもあり得るので、これらの方程式は無効になります。これがデータ漏洩の例です。
データ漏洩の問題は、トレーニング中に自己回帰アプローチまたはシーケンシャルアプローチを使用していれば発生しなかったでしょう。
したがって、これは並列アプローチを使用するときに直面する問題です。
-

Now we are stuck
When we used auto regressive approach, our training time was too slow but there was no problem of data leakage
When we used parallel approach, our training time was fast but there was problem of data leakage
Now do we have any solution which could bring best of both these situation like the training time decreases and there is no problem of data leakage also
Yes, and this is where Masked Multi Head attention comes into picture

Lets take the sentence “AAP KAISE HAI” and lets look what happens to this sentence when it goes inside self attention layer

--

さて、私たちは行き詰まっています。
自己回帰アプローチを使用した場合、トレーニング時間は非常に遅くなりましたが、データ漏洩の問題はありませんでした。
並列アプローチを使用した場合、トレーニング時間は速くなりましたが、データ漏洩の問題がありました。
この2つの状況の両方の利点をもたらす解決策はありますか？トレーニング時間が短縮され、データ漏洩の問題もない場合です。
はい、そしてここでマスク付きマルチヘッドアテンションが登場します。
文「AAP KAISE HAI」を取り上げ、この文が自己注意層に入るとどうなるかを見てみましょう。

-

